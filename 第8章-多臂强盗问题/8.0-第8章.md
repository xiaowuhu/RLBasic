# 第四步 蒙特卡洛与时序差分

# 第 8 章  探索与利用的平衡

【温故知新】从动态规划升级到蒙特卡洛。

多臂强盗（赌博机）问题很独特，在 Sutton 的书中，把它放在了第二章；而在 David Silver 的教程中，把它放在了最后一章。在本书中为什么把它放在了第 8 章？

首先，我们前面学习过的奖励、动作、策略等等，还有第 4 章中牛刀小试的蒙特卡洛采样法，在这里都可以用到。我们将从**动态规划**的世界来到**蒙特卡洛**的世界。

其次，截止到第 7 章，讲的都是与**有模型**（model-based）的、与动态规划相关的强化学习环境问题。而从第 9 章开始，将进入了**无模型**（model-free）的环境问题。而多臂赌博机问题的特点是：有模型（因为内含概率模型），无状态转移（因为智能体和环境每次只交互一下就立刻得到反馈），有奖励（否则没有研究的价值），有动作和策略（这是第 6 章才接触的概念），需要研究最优策略（这是第 7 章的内容）。

最后，它所用到的算法比较奇特，因为没有状态，完全和前面学习的贝尔曼方程没关系，也和后面将要学习的时序差分没关系。但是，读者可以把它想象为每搬动一次拉杆就是一个状态并且要连续搬动很多次，这样就可以和状态挂钩了。虽然状态之间没有转移概率，但是仍需要算法来决策拉动哪个拉杆。
