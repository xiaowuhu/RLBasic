
## 12.2 算法评测

可以有三类方法对强化学习的算法进行评测，下面分别介绍。

### 12.2.1 按效果衡量

可以根据奖励、分幕长度、获胜率、样本复杂度等指标进行评测，简单介绍如下。

#### 1. 奖励（收益获得能力）

强化学习的目标就是要训练一个有策略 $\pi$ 的智能体，让它从环境中获得的奖励的期望值最大化，公式如下所示。

$$
\pi_*=\arg\max_\pi J(\pi)
\\
J(\pi) = \int_t P(t \mid \pi) R(t) = \mathbb{E}_{t \sim \pi}[R(t)]
$$

如果随着训练，每幕平均奖励不断上升，就说明智能体能够学到东西。如果想找一个作为基准的奖励期望的话，可以让智能体一直做随机动作来获得。


#### 2. 分幕长度

记录每幕的长度。如果是探索性问题，每幕越短说明策略越好。如果是交互、对战类问题，以失败为结束条件，那么每局连续的回合越多越好。

#### 3. 获胜率

如果做的任务是有明确的胜负指标，可以记录一下该指标。比如一个空战游戏，每一步的奖励可以是我方机头和敌方飞机方向的夹角，夹角越小更容易打到对方，奖励也会越大。但是虽然奖励可能越来越大，仍然有可能最后胜率上不去。比如在国际象棋中吃掉对方的王后获得高额奖励，但因此输掉整盘棋。我们最终想要的是胜率提高。或者是奖励并没有提高很多，但是胜率上去了，比如飞机学到了奇葩的战斗策略，把对方飞机逼出界也可以获胜。

#### 4. 样本复杂度

采集样本需要时间，而在有些问题中，不可能采集到所有样本。比如自动驾驶，总会遇到不同的场景。如果能用较少的样本和较简单的样本就可以完成学习任务，则此算法是我们需要的。比如 Q 学习方法比 MC 方法更受青睐。

### 12.2.2 按时间衡量

可以根据步数或时间来衡量算法性能，简单介绍如下。

#### 1. 全局交互步数

这个指标可以比较准确的表示在一定时间内智能体收集了多少经验。这通常比现实时间更准确一些，花费的现实时间还会受硬件资源使用情况影响。

#### 2. 神经网络训练步数

对于异策略强化学习算法，需要不断从经验回放池中取出数据，训练神经网络，如下公式代表了训练强度（training intensity），表示每个样本平均被取出多少次进行神经网络训练，这个值应该远大于1，比如256。

$$
\text{rate}=\text{BatchSize} * 训练次数 / 总的时间步
$$

#### 3. 现实时间

训练需要多长时间，这对未来进行工作计划是有用的。

### 12.2.3 按其它指标衡量

其它衡量指标包括价值函数值、策略熵、KL散度等，简单介绍如下。

#### 1. 价值函数的值

对于针对价值函数的学习来说，可以看值函数的值（预测的未来奖励的期望）随着训练是否不断上升（每幕的每个时间步的值函数输出取平均），我们的训练目标就是不断提升值函数的值。

#### 2. 学习到的策略的熵

$$
H(p)=a p(a) \ln (p(a))
$$

熵表示了一个系统的随机程度，熵$H(p)$越大，随机程度越大,其中$a$是一个动作，$p(a)$是生成一个动作的概率。最大的熵值等于$\ln(N)$，其中$N$是动作的数量，它意味着智能体是均匀地随机选择动作的。最小熵值等于0，它意味着总是只会选择一个动作。如果观察到智能体的熵迅速下降，这是个不好的迹象。这意味着智能体很快就停止探索了。如果使用的是随机策略，应该考虑一些熵的正则化方法。如果使用的是确定性策略 + $\epsilon$-贪婪探索性策略，可以让随机动作概率衰减的慢一点。

#### 3. KL 散度

$$
\text{KL}(P|Q) = \sum p(x) \log \frac{p(x)}{q(x)}
$$

对于同策略方法，智能体的行动和学到的东西有很大的关系。如果学习率太大，模型的更新方向可能朝当前“看起来”比较好的方向更新太多（$p$ 与 $q$ 的差距太大），以至于可能达到一个不好的点，所以每次更新模型的步幅不能太大。KL 散度是用来衡量分布差异的指标，可以用 KL 散度来观察是不是更新的步幅太大。
